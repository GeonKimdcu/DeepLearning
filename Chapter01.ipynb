{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNB/OsP9QQEjkMcRyQlPbUn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeonKimdcu/DeepLearning/blob/main/Chapter01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter01 딥러닝 개요\n",
        "---"
      ],
      "metadata": {
        "id": "qyHVH_ePUH5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 딥러닝이란?\n",
        "\n"
      ],
      "metadata": {
        "id": "hL__zwmqUJIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.1 인공지능, 머신러닝과 딥러닝 정의"
      ],
      "metadata": {
        "id": "fxjN7XFzVU8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**인공지능**(AI, Artificial Intelligence)은 <font color='#ff6f61'>'문제를 인식하고 해결하는 능력인 지능을 구현하는 기술'</font>을 말한다. <br>\n",
        "\n",
        "**머신러닝**(ML, Machine Learning)은 <font color='#ff6f61'>'기계 스스로 학습하여 지능을 습득하는 기술'</font>을 말한다. 머신러닝은 학습 알고리즘을 통해 데이터에 숨겨진 정보와 규칙을 기계 스스로 습득하고 그 결과를 이용해서 새로운 것을 예측하고 추론하는 기술이다. <br>\n",
        "\n",
        "**딥러닝**(DL, Deep Learning)은 <font color='#ff6f61'>생체 신경망을 모방해서 만든 인공 신경망(ANN, Artificial Neural Network)을 이용하여 복잡한 데이터 관계를 찾아내는 머신러닝 기법</font>이다. 인공 신경망이 깊은 신경망(Deep Neural Network)으로 발전하여 '딥러닝'이라는 이름이 붙었다."
      ],
      "metadata": {
        "id": "sFmp6TQ8VBRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img width=\"550\" alt=\"스크린샷 2021-12-19 오후 3 25 12\" src=\"https://user-images.githubusercontent.com/48666867/146666140-1725c1ac-fffb-4dff-bf3e-ab0ff90b3452.png\">\n",
        "\n",
        "Ref. *https://blog.lgcns.com/2212*"
      ],
      "metadata": {
        "id": "qBEBXf1RXQ4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.2 머신러닝과 딥러닝의 관계"
      ],
      "metadata": {
        "id": "XPT7iKGFYV7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "전통적인 머신러닝 기법들은 특정한 문제에 맞게 알고리즘이 특화했다. <br>\n",
        "하지만 딥러닝 모델은 데이터의 복잡한 관계를 잘 표현하기 때문에 다양한 문제에 보편적으로 사용할 수 있다. 그래서 **작은 규모의 단순한 문제를 풀 때는 기존 머신러닝 기법으로 빠르게 해결**하고, 기존 머신러닝 기법으로 해결할 수 없는 수준으로 문제가 커지고 복잡해지면 딥러닝 모델을 적용할 수 있을지 검토해 보는 것이 좋다. <br>"
      ],
      "metadata": {
        "id": "I75s6QE8fIf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "머신 러닝 기법은 **지도 학습**(supervised learning), **비지도 학습**(unsupervised learning), **강화 학습**(reinforcement learning)으로 분류하는데 딥러닝은 이 중 특정 분야에 종속되지 않고 모든 분야에 범용적으로 적용될 수 있다. <br>\n",
        "\n",
        "지도 학습과 비지도 학습을 분류하는 기준은 **학습 과정에 전문가가 개입하여 머신러닝 모델의 학습을 지도하는지 여부**이다. <br>\n",
        "\n",
        "지도 학습 방법은 <font color='#ff6f61'>입력 데이터에 대한 정답을 사전에 정의해서 학습 데이터로 제공하는 것</font>으로, 모델은 자신의 예측과 정답의 차이를 보고 오차를 줄이도록 학습한다. <br>\n",
        "\n",
        "대표적인 지도 학습에는 데이터를 클래스별로 **분류**(classification)하거나 데이터의 함수적 관계를 알아내는 **회귀**(regression)가 있다.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "✨머신러닝에서 모델은 다양한 변수 간의 수학적/확률적 관계를 표현하는 자료구조와 알고리즘 또는 프로그램을 말한다. <br>\n",
        "✨지도 학습에서 학습 데이터를 통해 제공하는 입력 데이터의 정답을 **타깃**(target) 또는 **레이블**(label)이라고 한다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HPyl9GqagRtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "비지도 학습은 <font color='#ff6f61'>전문가의 개입 없이 순수하게 데이터만으로 학습하는 방법</font>이다. <br>\n",
        "\n",
        "비슷한 데이터끼리 묶어주는 **클러스터링**(clustering), 고차원 데이터를 저차원 데이터로 변환하는 **차원 축소**(dimension reduction), 데이터의 핵심적인 정보를 낮은 차원의 잠재 데이터로 표현하는 **표현 학습**(representation learning), 새로운 데이터를 생성하는 **데이터 생성**(data generation), 데이터 간의 규칙을 찾아내는 **연관 규칙**(association rule), 사람의 행동을 분석하여 관심 항목을 추천하는 **협업 필터링**(collaborative filtering) 등이 비지도 학습에 속한다."
      ],
      "metadata": {
        "id": "pTjabWrUkQ2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "강화 학습은 <font color='#ff6f61'>순차적인 의사 결정 문제</font>(sequential decision problem)를 다룬다. <br>\n",
        "\n",
        "즉, 강화 학습은 현재의 의사 결정이 미래에 영향을 미칠 때 목표를 달성하기 위한 순차적인 의사 결정 방법을 학습한다. <br>\n",
        "'보상과 강화가 행동의 형성 과정에 큰 영향을 미친다'는 행동강화 이론을 토대로 **보상을 최대화하는 행동을 선택**하도록 학습하는 방식이다. <br>\n",
        "\n",
        "강화 학습은 비디오 게임이나 보드게임 분야에서 높은 성능을 보이며, 그 밖에도 센서 정보를 이용한 간단한 기기 및 로봇 제어, 자율 주행과 같은 영역에서 전문가의 행동을 따라하는 모방 학습, 추천 시스템과 같은 분야에도 활용된다."
      ],
      "metadata": {
        "id": "cV2CqlDAlIZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img width=\"550\" src=\"https://user-images.githubusercontent.com/48666867/146667480-54e562f0-5148-4195-a481-9f25c03994ea.jpg\">\n",
        "\n",
        "Ref. *Do it! Deep Learning Textbook*"
      ],
      "metadata": {
        "id": "_8ZJV9jioPQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.3 딥러닝의 장점과 한계"
      ],
      "metadata": {
        "id": "IzPz79MNo7C7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**딥러닝의 장점**\n",
        "\n",
        "\n",
        "1.   **함수를 근사하는 능력이 뛰어나다.** <br>\n",
        "딥러닝은 범용적인 함수 근사 능력을 갖추고 있으므로 기존 머신러닝 기법으로는 한계가 있는 클래스 간의 경계가 아주 복잡한 데이터를 분류하거나 복잡한 형태의 비선형 함수(nonlinear function)를 근사할 때 매우 뛰어난 성능을 보인다.\n",
        "\n",
        "2.   **특징을 자동으로 추출한다.** <br>\n",
        "기존 머신러닝 기법을 사용할 때는 도메인 전문가가 모델에 입력할 특징(feature)을 설계하고 데이터에서 직접 특징을 추출(feature extraction)해야 한다. 그러므로 만약 특징이 잘못 설계되면 모델 성능이 떨어질 수 밖에 없다. <br>\n",
        "하지만 인공 신경망은 중요한 특징에는 높은 가중치(weight)를, 중요하지 않은 특징에는 낮은 가중치를 부여함으로써 특징을 자동으로 추출한다. 즉, 사람의 개입 없이 특징에 의미 부여가 가능하다. <br>\n",
        "이를 통해 사람의 편견 또는 편향(bias)으로 생기는 오류를 배제할 수 있다. 또한 여러 단계로 진행하던 작업을 사람의 개입 없이 종단간(End-to-End)으로 진행할 수 있게 되면서 추론 시간이 빨라지고 추론 성능도 높아진다.\n",
        "\n",
        "3.   **모델의 확장성이 뛰어나다.** <br>\n",
        "딥러닝 모델은 뉴런 연결로 구성되므로 복잡한 문제를 해결하기 위한 큰 모델을 만들려면 뉴런 수를 늘려주면 된다. 또한 뉴런을 실행할 때 병렬처리가 가능한 구조로 되어 있어서 모델을 대규모로 확장하기 쉽다.\n",
        "\n",
        "4.   기존 머신러닝보다 훨씬 좋은 성능을 보인다. <br>\n",
        "딥러닝은 이미지 분류, 객체 탐지 등의 비전 분야에서 이미 인간의 인지 능력을 뛰어넘는 성능을 보여주었으며, 특히 최근에는 이미지 처리, 자연어 처리, 음성 인식과 같은 분야에서의 성능적 발전이 급격히 이루어지면서 광범위한 분야에 실용적으로 접목되고 있다.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JybAN_3lo_l0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**딥러닝의 한계**\n",
        "\n",
        "\n",
        "1.   **딥러닝 모델은 파라미터(parameter)가 많기 때문에 다른 머신러닝 모델보다 상대적으로 많은 학습 데이터가 필요하다.** <br>\n",
        "기존 머신러닝 기법은 특정 용도로 설계되어 있어서 적은 데이터로도 학습이 되지만, 성능적 한계가 존재하기 때문에 데이터 양이 늘어나도 성능이 더 좋아지지 않는다. <br>\n",
        "반면, 딥러닝은 범용적이기 때문에 기본적으로 모델 파라미터가 많고 학습 데이터가 많이 필요하지만, 데이터 양이 증가할수록 모델의 크기를 늘려줄 수 있으므로 성능도 그에 비례하여 향상한다.\n",
        "\n",
        "2.   **딥러닝 모델은 훈련을 위한 시간과 비용이 많이 든다.** <br>\n",
        "훈련 과정에서 데이터가 많이 필요하기 때문에 훈련 시간과 컴퓨팅 자원도 데이터 양에 비례하여 증가할 수 밖에 없다. <br>\n",
        "훈련 시간과 비용을 줄이고 효율을 높이기 위해 매번 처음부터 학습하지 않고, 대신 대용량 데이터로 사전에 학습된 모델을 이용해서 빠르게 학습하는 **전이 학습**(transfer learning)방법이 연구되고 있다.\n",
        "전이 학습을 하면 적은 양의 데이터로 모델을 빠르게 튜닝해서 새로운 목적에 맞게 사용할 수 있으며 동시에 모델의 성능까지 보장할 수 있다. <br>\n",
        "또한 인간과 같이 빠르게 학습하는 능력을 만들기 위해 여러 작업(task)의 **학습 방식을 학습**(learn to learn)한 후에 유사한 작업을 적은 데이터로 빠르게 학습하는 **메타 학습**(meta learning)도 점점 중요해지고 있다. <br>\n",
        "그 밖에, 한번 학습한 내용을 잊지않고 계속해서 새로운 것을 누적해서 학습해 나가는 **평생 학습**(lifelong learning), 성능에 큰 영향을 미치는 데이터를 선별하여 학습함으로써 선별된 적은 데이터로 빠르게 학습하는 **액티브 학습**(active learning)방식의 연구들이 활발히 진행되고 있다.\n",
        "\n",
        "3.   딥러닝 모델에는 설정 파라미터가 많아서 최적의 모델과 훈련 방법을 찾으려면 상당히 많은 검색 시간과 튜닝 시간이 필요하다. <br>\n",
        "따라서 최적의 딥러닝 모델을 자동으로 찾아주는 신경망 구조 탐색(NAS:neural architecture search)연구 개발이 활발히 진행되고 있다. <br>\n",
        "Google이 개발한 AutoML은 강화 학습 방식으로 최적의 모델을 생성하는 방법을 학습한다. 컨트롤러가 다양한 모델을 제안하면 제안된 모델을 실행해서 정확도를 평가하고 이를 강화 학습의 보상으로 사용해서 컨트롤러를 학습하는 방식이다.\n",
        "\n",
        "   <img width=550 src=https://user-images.githubusercontent.com/48666867/146668707-654af458-6513-4f7f-9af3-ae602c827ec8.png>\n",
        "   \n",
        "   Ref. *https://www.fast.ai/2018/07/16/auto-ml2/*\n",
        "\n",
        "4.   인공 신경망 모델은 오류를 파악하거나 디버깅하기 어렵다. <br>\n",
        "이를 보완하기 위해 개발과정의 편의를 돕는 프레임워크나 자동화, 시각화, 모니터링에 필요한 도구 개발이 활발히 진행되고 있다. 또한 **설명 가능한 인공지능**(XAI:explainable AI)분야에서는 신경망 모델 내부에서 일어나는 내용을 확인할 수 있는 기법들을 연구하고 있다. <br>\n",
        "즉, 딥러닝 모델 기반 서비스의 관리자나 사용자들이 인공 신경망이 어떻게 작동하고 무엇을 학습했는지, 어떤 입력이 출력에 큰 영향을 미치는지와 같은 내용을 이해할 수 있도록 정량화하고 시각화한다.\n",
        "\n",
        "5.   지도 학습에서는 타깃 데이터를 만들 때 드는 비용이 만만치 않다. <br>\n",
        "그러므로 **생성 모델**(generative model)을 이용하여 훈련 데이터를 자동으로 생성해내는 방법과 함께, 사람이 제공하는 힌트 데이터나 타깃 데이터를 이용하여 자동으로 타깃을 만드는 **준 지도 학습**(semi-supervised learning)및 학습 과정에 사람이 개입하지 않아도 되는 **자기 지도 학습**(self-supervised learning)방식에 관한 관심이 커지고 있다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3sRSJuiVxsOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 딥러닝의 역사"
      ],
      "metadata": {
        "id": "5GN7ywHE2Hx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1943년에 최초의 인공 신경망이 만들어지고 1956년에는 스스로 학습하는 인공 신경망인 '퍼셉트론(perceptron)'이 세상에 등장했다. <br>\n",
        "딥러닝이란 용어는 2006년에 이르러서야 비로소 등장한다. 인공 신경망의 역사에 지금까지 2번의 암흑기가 있었지만, 딥러닝이 제안된 이후 인공지능 기술은 매우 빠른 속도로 발전하고 있으며 새로운 시대적 변화를 주도하고 있다.\n",
        "\n",
        "<img width=800 src=https://user-images.githubusercontent.com/48666867/146669016-cf4d19c7-2fc3-44cb-acd9-db3c84d5a9eb.png>\n",
        "\n",
        "Ref. *Do it! Deep Learning Textbook*"
      ],
      "metadata": {
        "id": "v3lzyECq2a9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.1 최초의 인공 신경망: McCulloch-Pitts Model"
      ],
      "metadata": {
        "id": "VXUPcnrn4JJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**McCulloch-Pitts** 모델은 '최초의 인공 신경망' 모델로서 <font color='#ff6f61'>인간의 신경계를 이진 뉴런</font>으로 표현하려고 했다. <br>\n",
        "활성 상태와 비활성 상태를 갖는 **이진 뉴런**(binary neuron)으로 구성되며 생체 뉴런과 같이 시냅스의 흥분과 억제에 따라 신호가 전달되고 특정 임계치를 넘어야 신호가 발화한다."
      ],
      "metadata": {
        "id": "7Bkgx17G4Ro5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 학습하는 인공 신경망: 퍼셉트론"
      ],
      "metadata": {
        "id": "uuulkRNm5Hz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "McCulloch-Pitts 모델은 학습 과정이 없어 문제에 따라 신경망의 구성도 매번 바꿔야 하는 단점이 있다. <br>\n",
        "이를 해결하기 위해 Frank Rosenblatt은 인공 신경망이 스스로 문제에 맞춰 학습하는 모델인 **퍼셉트론**(perceptron)을 개발했다. <br>\n",
        "퍼셉트론의 학습 알고리즘은 <font color='#ff6f61'> 새로운 입력에 대한 오차가 발생하면 뉴런의 연결 강도를 조절하는 방식</font>이다.\n",
        "\n",
        "<img src=\"https://i.imgur.com/EtpSmaG.png\" width=\"600\">"
      ],
      "metadata": {
        "id": "ANXeL3hF5MhE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "퍼셉트론은 입력 데이터가 들어오면 가중치와 곱해서 가중 합산을 하며 그 결과가 0보다 크면 1을 출력하고 그렇지 않으면 0을 출력한다. 즉, 가중 합산과 계단 함수(step function)를 순차적으로 실행하는데 이때 계단 함수는 퍼셉트론의 활성 여부를 결정한다. 그래서 계단 함수를 **활성 함수**(activation function)라고 부른다.\n",
        "\n",
        "퍼셉트론은 다음 그림과 같이 두 종류의 클래스를 직선으로 분류하는 **선형 분류기**(linear classifier)로, 입력 데이터와 가중치의 가중 합산 식은 두 클래스를 분류하는 **결정 경계**(decision boundary)를 이룬다.\n",
        "\n",
        "<img width=450 src=https://user-images.githubusercontent.com/48666867/146669888-d3319c3f-9305-4f02-b43d-29cee6f69632.png>\n",
        "\n",
        "Ref. *https://starcell.github.io/ai/dl-basic/*\n",
        "\n",
        "즉, 퍼셉트론의 가중 합산이 0인 $z = w_1x_1+w_2x_2+b=0$ 식은 결정 경계를 이루는 직선의 방정식으로 가중치 $(w_1, w_2)$는 직선의 법선 벡터(normal vector)를, 편향 $b$는 원점과 직선 사이의 거리를 나타낸다. <br>\n",
        "만일 $b=0$ 이면 직선이 항상 원점을 지나므로 임의의 위치에 있는 두 종류의 클래스를 완벽히 분류할 수 없다. <br>\n",
        "직선을 경계로 법선 벡터 방향에 있는 점들은 계단 함수의 실행 결과가 0이 되어 클래스 2로 분류된다.\n",
        "\n",
        "\n",
        "---\n",
        "✨입력과 가중치의 가중 합산이 0인 식이 $w_1x_1+w_2x_2+b=0$ 와 같이 2차원이면 직선 방정식이 되지만 $z=w_1x_1+w_2x_2+w_3x_3+b=0$과 같이 3차원이면 평면 방정식이 되고 $w_1x1_+w_2x2+...+w_nx_n+b=0$과 같이 $n$차원이면 **초평면**(hyperplane)방정식이 된다. 이들은 모두 공간을 둘로 나눈다는 특징이 있다.\n"
      ],
      "metadata": {
        "id": "hQjchTe-6bdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **<font color='#ff6f61'>가중치(Weight)-편향(bias) 연산</font>**<br/>\n",
        "위에서 알아본 것처럼 퍼셉트론은 여러 개의 신호를 입력받는다.<br/>\n",
        "입력된 신호는 각각의 가중치와 곱해지고 그 결과를 더해주게 된다.<br/>\n",
        "이러한 **가중합(Weighted Sum)**이 퍼셉트론의 첫 번째 단계이다..\n",
        "\n",
        "    간단한 예시 코드를 통해 가중합이 어떻게 일어나는지 알아보자."
      ],
      "metadata": {
        "id": "jK_vpYZWUqj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "input = np.array([1, 2, 3])\n",
        "weight = np.array([0.2, 0.3, -0.1])\n",
        "\n",
        "np.dot(input, weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esA4TrFzXpgo",
        "outputId": "30a724ae-1efb-4105-b383-9021e86b4b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 식에서 입력 신호(**`input`**) $[1, 2, 3]$ 에 대해 임의의 가중치(**`weight`**) $[0.2, 0.3, -0.1]$ 가 설정되어 있는데요.<br/>\n",
        "코드가 수행하는 계산은 다음과 같습니다.<br/>\n",
        "\n",
        "$$\n",
        "1 \\times 0.2 + 2 \\times 0.3 + 3 \\times (-0.1) = 0.5\n",
        "$$\n",
        "\n",
        "<br/>\n",
        "\n",
        "이를 일반화 시키면 입력 신호 $(x_0, x_1, \\cdots)$와 가중치$(w_0, w_1, \\cdots)$에 대하여 다음과 같은 식으로 나타낼 수 있습니다.\n",
        "\n",
        "$$\n",
        "\\sum(b + w_0x_0 + w_1x_1 + ... + w_nx_n)\n",
        "$$\n",
        "\n",
        "<br/>"
      ],
      "metadata": {
        "id": "JlwqQDR4U9Tv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.3 논리 게이트로 퍼셉트론 알아보기"
      ],
      "metadata": {
        "id": "p5sq0hwIXaUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "퍼셉트론의 가장 단순한 형태는 AND, NAND, OR 과 같은 **논리 게이트(Logic Gate)**이다.<br/>\n",
        "논리 게이트의 예시를 알아보고 구현해 보면서 퍼셉트론을 좀 더 잘 이해해보자."
      ],
      "metadata": {
        "id": "e-43nWQhXeuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AND GATE\n",
        "\n",
        "> **AND GATE 는 입력 신호가 모두 1(True)일 때 1(True)을 출력**한다.\n",
        "\n",
        "<br/>\n",
        "\n",
        "실제로 AND GATE 를 사용하는 예시는 어떤 것이 있을까? \n",
        "\n",
        "- 예) 밤에 라면을 먹을지 말지를 고민하는 상태라고 해보자.<br/> \n",
        "    - (1) 저녁을 안 먹었는가?\n",
        "    - (2) 11시 이전인가?\n",
        "\n",
        "두 조건을 만족할 때만 라면을 먹는 ~~(엄격한)~~ 사람이라면, AND GATE 처럼 동작한다고 할 수 있다.<br/>\n",
        "아래 진리표를 보며 각 경우에 AND GATE 가 어떻게 동작하는지 알아보도록 하자.\n",
        "\n",
        "> ❗️ ***아래 등장하는 게이트의 기호는 학계의 약속이기 때문에 그대로 받아들이시면 된다.***"
      ],
      "metadata": {
        "id": "V9wEEQM5Xp5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.imgur.com/uMFnwaf.png\" width=\"400\"/>"
      ],
      "metadata": {
        "id": "Nxa2rZ3GYJLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NAND GATE\n",
        "\n",
        "> **NAND는 Not AND 의 줄임말로 AND GATE의 결과의 반대를 출력**한다.\n",
        "\n",
        "아래 진리표를 보며 각 경우에 NAND GATE 가 어떻게 동작하는지 알아보도록 하자."
      ],
      "metadata": {
        "id": "OC6OWrvAYLXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.imgur.com/6MiCoFk.png\" width=\"400\"/>"
      ],
      "metadata": {
        "id": "TN-tkkhDYVOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OR GATE\n",
        "\n",
        "> **OR GATE 는 입력 신호 중 하나만 1(True)이라도 1(True)을 출력**한다.\n",
        "\n",
        "아래 진리표를 보며 각 경우에 OR GATE 가 어떻게 동작하는지 알아보도록 하자."
      ],
      "metadata": {
        "id": "dJU8X4W1Ybcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.imgur.com/xG3OLEI.png\" width=\"400\"/>"
      ],
      "metadata": {
        "id": "vnkPkLewYf6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### XOR GATE\n",
        "\n",
        "> **XOR GATE 는 배타적 논리합(Exclusive-OR)이라고도 불리는 GATE 입니다. 입력 신호가 다를 경우 1(True)을 출력**한다."
      ],
      "metadata": {
        "id": "T83nan27YmmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.imgur.com/CsnYMeV.png\" width=\"400\"/>"
      ],
      "metadata": {
        "id": "A78nB4ejYtEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "신경망이 논의되던 초기에 **퍼셉트론의 한계로 지적되었던 것이 바로 <font color=\"ff6f61\">XOR GATE 의 표현</font>**이었다."
      ],
      "metadata": {
        "id": "_JMjgRPwYxzM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래는 각각 AND, OR, XOR의 진리표를 나타내고 있는 그림이다.<br/>\n",
        "**검은 색이 1(True), 흰 색은 0(False)**를 나타낸다.<br/>\n",
        "그리고 AND와 OR에 해당하는 그림에는 두 클래스(True, False)를 분류하기 위한 분류 경계를 나타낸다.\n",
        "\n",
        "> ❗️ ***XOR 에서 선형 경계로 두 클래스를 제대로 분류할 수 있을까?<br/>***\n",
        "\n",
        "<img src=\"https://i.imgur.com/H79z1il.png\" width=\"600\"/>"
      ],
      "metadata": {
        "id": "5NQu38gkY22H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ❗️ ***그렇다면 2개 이상의 경계를 사용하면 XOR 문제를 선형 분류기로 해결할 수 있을까?<br/>***\n",
        "\n",
        "<img width=\"400\" src=\"https://user-images.githubusercontent.com/48666867/146672965-25a23d16-c075-4df1-8b6a-33bf06fbd4fa.png\">\n",
        "\n",
        "Ref. *https://yceffort.kr/2019/02/20/pytorch-04-deep-neural-network*\n"
      ],
      "metadata": {
        "id": "baY6oqNvZ8d4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "두 직선을 정의하는 첫 번째 계층과 AND 연산을 하는 두 번째 계층으로 구성되는 인공 신경망을 정의해야 이 문제를 해결할 수 있다. 하지만 퍼셉트론은 하나의 계층만 표현할 수 있기 때문에 이 문제를 풀 수 없다.<br>\n",
        "따라서 퍼셉트론으로 비선형 문제를 풀려면 여러 계층을 표현하는 **다층 퍼셉트론**(MLP:multi layered perceptron)으로 확장되어야 한다."
      ],
      "metadata": {
        "id": "HQs74jQxamlK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.4 역전파 알고리즘의 발견"
      ],
      "metadata": {
        "id": "7wPhdGUsbG-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "인공 신경망은 학습 과정에서 출력과 정답의 오차를 최소화하도록 **최적화**(optimization)를 수행한다. 이때 각 파라미터의 오차에 대한 기여도를 미분으로 계산해서 오차를 최소화하는 방향으로 파라미터를 조정한다. <br>\n",
        "\n",
        "역전파 알고리즘은 신경망의 뉴런에 분산된 파라미터의 미분을 효율적으로 계산하기 위한 알고리즘이다. <br>\n",
        "**출력 계층**(output layer)에서 **입력 계층**(input layer)방향으로 한 계층씩 이동하면서 미분을 계산하고 다음 계층에 전파하는 과정을 통해 계산의 중복 없이 모든 파라미터의 미분을 한 번의 패스에 계산한다.\n",
        "\n",
        "<img width=600 src=https://user-images.githubusercontent.com/48666867/146673170-1bc724f7-bc60-4a78-9a85-d25f5ff66549.png>\n",
        "\n",
        "Ref. *https://ds-uno-blog.netlify.app/2020/06/04/backpropagation-neural-network/*\n",
        "\n",
        "역전파 알고리즘을 통해 다층 퍼셉트론을 학습시킬 수 있게 되면서 그동안 퍼셉트론으로는 풀 수 없었던 비선형 문제를 풀 수 있게 되었다.\n",
        "\n",
        "<img width=400 src=https://user-images.githubusercontent.com/48666867/146673232-9b47c6a6-0a08-435d-b1c3-1406980fcdc4.png>\n",
        "\n",
        "Ref. *https://tex.stackexchange.com/questions/376766/plotting-points-around-two-circles-using-tikz*"
      ],
      "metadata": {
        "id": "t3SW7IUZbLwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.5 딥러닝 시대를 열다"
      ],
      "metadata": {
        "id": "UbX4xSOecEHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "역전파 알고리즘이 발견된 이후에도 인공 신경망은 여전히 잘 활용되지 못했다.<br>\n",
        "인공 신경망의 학습을 어렵게 만드는 과적합과 그레디언트 소실 문제가 해결되지 않았기 때문이다."
      ],
      "metadata": {
        "id": "EEBg1kaHcx17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 모델이 데이터를 암기하는 현상: 과적합\n",
        "\n",
        "아래 그림과 같이 점으로 표현된 관측 데이터가 있다. <br>\n",
        "이 관측 데이터를 가장 잘 설명하는 곡선은 가운데 그래프와 같이 관측 데이터의 평균 지점을 지나는 부드러운 곡선이다.<br>\n",
        "모델이 관측 데이터를 잘 설명하는 곡선을 표현할 때 잘 적합(good fitting)되었다고 말한다.\n",
        "\n",
        "<img width=500 src=https://user-images.githubusercontent.com/48666867/146673318-7a26354e-fb5e-43ad-b8c5-cb753ada294d.png>\n",
        "\n",
        "Ref. *educative.io*"
      ],
      "metadata": {
        "id": "xgw7nLD6c927"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "오른쪽 그래프와 같이 모델이 모든 점을 지나는 복잡한 모양의 곡선을 표현할 때 **과적합**(overfitting)했다고 말한다. <br>\n",
        "쉽게 말해 과적합은 모델이 과도하게 학습되어 '데이터를 암기'한 상태로, 모델 분산이 커졌기 때문에 관측 데이터는 정확히 예측하지만 새로운 데이터는 정확히 예측하지 못한다. <br>\n",
        "\n",
        "과적합의 주요 원인은 학습 데이터보다 모델의 파라미터 수가 많기 때문이다. 그래서 모델 파라미터가 많은 인공 신경망은 다른 모델보다 과적합되기 쉽다."
      ],
      "metadata": {
        "id": "s0wzLSXmdqQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "왼쪽 그래프는 모델이 표현하는 곡선이 데이터의 분포를 제대로 반영하지 못하는 **과소적합**(underfitting)한 상태를 보여준다. <br>\n",
        "모델이 작거나 단순해서 데이터를 잘 표현하지 못하면 과소적합 상태가 된다. 이럴 때는 모델의 크기를 키우거나 표현력이 좋은 모델로 변경해야 한다.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "✨ 인공 신경망에서 모델 파라미터는 가중치나 편향과 같이 학습을 통해 값이 정해지는 파라미터를 말한다.<br>\n",
        "✨ 반면 학습 전에 미리 설정하는 파라미터는 **하이퍼파라미터**(hyperparameter)라고 구분하여 부른다."
      ],
      "metadata": {
        "id": "T2ujbeJueJuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 학습이 중단되는 현상: 그레디언트 소실\n",
        "\n",
        "**그레디언트 소실**(gradient vanishing)은 깊은 신경망을 학습할 때 역전파 과정에서 미분값이 사라지면서 학습이 중단되는 현상을 말한다.\n",
        "\n",
        "역전파 알고리즘은 출력 계층에서 입력 계층 방향으로 진행되는데, 각 계층에서 계산한 미분값이 0에 가까울수록 아주 작은 숫자가 여러 번 곱해지면서 결국 0으로 수렴하여 미분값이 사라진다. <br>\n",
        "\n",
        "신경망이 깊어질수록 이런 현상이 두드러지기 때문에 신경망 학습이 어려워진다.\n",
        "\n",
        "\n",
        "---\n",
        "✨입력이 $n$차원 벡터이고 출력이 실수인 $f:R^n->R$ 형태의 실함수의 1차 미분을 **그레디언트**(gradient)라고 한다. 인공 신경망의 미분을 그레디언트로 표현하는 이유는 인공 뉴런이 $f:R^n->R$ 형태의 함수이기 때문이다.\n",
        "\n",
        "<img width=550 src=https://user-images.githubusercontent.com/48666867/146673927-8da1ca56-bcda-4a0d-854e-c74e0121b216.png>\n",
        "\n",
        "Ref. *https://excelsior-cjh.tistory.com/177*"
      ],
      "metadata": {
        "id": "_hkTOmdye1Xq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 깊은 신경망을 안정적으로 학습시킬 수 있는 딥러닝\n",
        "\n",
        "인공 신경망의 학습을 어렵게 만드는 과적합과 그레디언트 소실 문제를 해결할 수 있도록 제프리 힌턴이 딥러닝 방법을 제시하였다."
      ],
      "metadata": {
        "id": "_dEAHSgsjTG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference\n",
        "\n",
        "1. Do it! 딥러닝 교과서"
      ],
      "metadata": {
        "id": "PvBO1_pMj_qs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "*made by* <font color='6699ff'>Kim Geon</font>(DCU, major.AI·BD)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "MTgrlz6Dkq0c"
      }
    }
  ]
}